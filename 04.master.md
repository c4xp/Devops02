---
currentMenu: 04.master
layout: default
title: Devops02
subTitle: Master
---

# The Master

Ping the new machine 

```
ping node-u4
echo "Did you forgot to add the machine to /etc/hosts ?"
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub | ssh peakit@node-u4 "mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys"
ssh -i $HOME/.ssh/id_rsa peakit@node-u4 docker version
```

## Installing Kubernetes command-line tool

Download and install the GPG key for Kubernetes packages.

```
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
apt-key list
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt update
apt install -y kubectl
```

## Install Nginx load balancer on another machine

- nginx LB layer4 with proxy-protocol
- DNS and domain needed to point to our loadbalancer (rancher.example.com)

```
apt update
apt upgrade
apt install -y nginx
```

```
cat <<EOF | sudo tee /etc/nginx/nginx.conf
worker_processes auto;
error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;
events {
    worker_connections  8192;
}
stream {
    upstream rancher_servers_http {
        least_conn;
        server 10.20.30.11:80 max_fails=3 fail_timeout=5s;
        server 10.20.30.12:80 max_fails=3 fail_timeout=5s;
        server 10.20.30.13:80 max_fails=3 fail_timeout=5s;
    }
    server {
        listen 80;
        proxy_protocol on;
        proxy_pass rancher_servers_http;
    }
    upstream rancher_servers_https {
        least_conn;
        server 10.20.30.11:443 max_fails=3 fail_timeout=5s;
        server 10.20.30.12:443 max_fails=3 fail_timeout=5s;
        server 10.20.30.13:443 max_fails=3 fail_timeout=5s;
    }
    server {
        listen 443;
        proxy_protocol on;
        proxy_pass rancher_servers_https;
    }
}
```

### Looking at an overview

Loadbalancer, Ingresses, Nodes and Pods

![Fusion reactor](https://raw.githubusercontent.com/c4xp/Devops02/master/assets/newrelic.jpg)

## Question 8

```
How many web-servers are there in the user path ?
```

# Break
```
10 minutes
```

## Question 9

```
Where do you prefer to store your data ?
```

## Install Helm

```
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod +x get_helm.sh
./get_helm.sh
```

## Install persistent NFS server storage

[Nfs Server How to](https://computingforgeeks.com/install-and-configure-nfs-server-on-ubuntu-debian/)

```
sudo apt update && sudo apt upgrade
sudo hostnamectl set-hostname nfs-server.example.com --static
sudo apt -y install nfs-kernel-server
sudo mkdir /srv/nfs/kubedata -p
sudo chown nobody:nogroup /srv/nfs/kubedata/
sudo systemctl enable --now nfs-server
sudo systemctl start nfs-server
sudo systemctl status nfs-server
sudo ufw allow from 10.20.30.0/24 to any port nfs
```

```
sudo vi /etc/exports
echo "/srv/nfs/kubedata *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)"
sudo exportfs -rav
```

## Install NFS client and test

```
sudo apt install nfs-common
sudo mount -t nfs 10.20.30.15:/srv/nfs/kubedata /mnt
mount | grep nfs
sudo umount /mnt
```

## On the master

```
helm repo add stable https://kubernetes-charts.storage.googleapis.com
kubectl create namespace nfs-client-provisioner
```

## Either Add app "nfs-client-provisioner" in the Rancher GUI
## Or add app "nfs-client-provisioner" with helm

```
helm repo update
helm install nfs-client-provisioner stable/nfs-client-provisioner --set nfs.server=10.20.30.15 --set nfs.path=/srv/nfs/kubedata --set storageClass.defaultClass=true --namespace nfs-client-provisioner
```

## Edit nfs-storage-class.yml

```
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: cluster.local/nfs-client-provisioner
parameters:
  archiveOnDelete: "true"
```

## Add the Storage Class

```
kubectl apply -f nfs-storage-class.yml
```

## Back to our Master

Edit cluster.yml

```
cat <<EOF | sudo tee ~/cluster.yml
nodes:
  - address: 10.20.30.11
    ssh_key_path: ./rancher.pem
    user: devops
    role: [controlplane,etcd,worker]
  - address: 10.20.30.12
    ssh_key_path: ./rancher.pem
    user: devops
    role: [controlplane,etcd,worker]
  - address: 10.20.30.13
    ssh_key_path: ./rancher.pem
    user: devops
    role: [controlplane,etcd,worker]
services:
  etcd:
    snapshot: true
    creation: 6h
    retention: 24h
  kubelet:
    extra_args:
      allowed-unsafe-sysctls: net.*

kubernetes_version: v1.24.4-rancher1-1

dns:
  provider: coredns
  upstreamnameservers:
  - 10.20.30.1

network:
  plugin: canal
  options:
    canal_iface: eth0
    canal_flannel_backend_type: vxlan

ingress:
  provider: "nginx"
  options:
    use-proxy-protocol: "true"
    use-forwarded-headers: "true"
    enable-brotli: "true"
    use-http2: "true"
    server-tokens: "false"
  node_selector: {}
  extra_args: {}

addon_job_timeout: 300
EOF
```

## Install rke

In Rancher v2.4+, it needs to be installed on a K3s Kubernetes cluster or an RKE Kubernetes cluster. [See](https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#1-install-the-required-cli-tools)

Visit [RKE releases](https://github.com/rancher/rke/releases)

```
wget https://github.com/rancher/rke/releases/download/v1.3.15/rke_linux-amd64
mv rke_linux-amd64 rke1315
chmod +x rke1315
./rke1315 config -l -a
sed -i -e 's~kubernetes_version:.*~kubernetes_version: v1.24.4-rancher1-1~' cluster.yml
./rke1315 up
./rke1315 etcd snapshot-save --name rke1315.db
```

Testing
```
cp kube_config_cluster.yml ~/.kube/config
kubectl get pods -A -o wide
```

## Cert Manager

If needed for installing rancher with letsencrypt

```
helm repo add jetstack https://charts.jetstack.io
helm repo update 

export CERTMTAG=$(curl -s https://api.github.com/repos/cert-manager/cert-manager/releases/latest|grep tag_name|cut -d '"' -f 4|sed 's/v//')

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v$CERTMTAG/cert-manager.crds.yaml

helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version $CERTMTAG
```

Testing
```
cat <<EOF | sudo tee test-certmanager.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager-test
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: test-selfsigned
  namespace: cert-manager-test
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: selfsigned-cert
  namespace: cert-manager-test
spec:
  dnsNames:
    - example.com
  secretName: selfsigned-cert-tls
  issuerRef:
    name: test-selfsigned
EOF

kubectl apply -f test-certmanager.yaml
kubectl describe certificate -n cert-manager-test

kubectl delete -f test-certmanager.yaml && rm -f test-certmanager.yaml
```

## Rancher

Use helm repo add command to add the Helm chart repository that contains charts to install Rancher

```
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
helm repo update

helm upgrade -i rancher rancher-stable/rancher --create-namespace --namespace cattle-system --set hostname=rancher.exmaple.com --set replicas=1
```

Monitor the installation

```
kubectl -n cattle-system rollout status deploy/rancher
```

Testing
```
kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}'

kubectl get ing -n cattle-system
kubectl describe svc rancher -n cattle-system
```

## Couple of warnings

[Snapshot docs](https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/)

```
echo "Why different binary names"
echo "Teardown the cluster and clean cluster nodes"
./rke remove
echo "Teardown the cluster and clean cluster nodes"
./rke etcd snapshot-save
```

## Persistent Volumes

```
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm repo update

helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=10.20.30.16 --set nfs.path=/volume1/clusterdata --set nfs.mountOptions="{nolock,hard}" --set storageClass.defaultClass=true
```

Testing
```
kubectl get pv -A -o wide
```

## Apply it to the cert-manager namespace
```
kubectl apply -f prod_issuer.yml --namespace cert-manager
kubectl get ClusterIssuer --all-namespaces
kubectl describe ClusterIssuer letsencrypt-examplecom --namespace cert-manager
```

## Questions

![Questions](https://raw.githubusercontent.com/c4xp/Devops02/master/assets/questions.jpg)

[Infrastructure as codeâ†’](05.infraascode.md)